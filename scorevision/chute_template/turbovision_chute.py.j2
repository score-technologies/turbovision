#!/usr/bin/env python3

from typing import Any, Generator
from tempfile import NamedTemporaryFile
from pathlib import Path
from importlib.util import spec_from_file_location, module_from_spec
import sys
from inspect import signature
from urllib.parse import quote

from aiohttp import ClientSession
from yaml import safe_load
from pydantic import BaseModel
from cv2 import VideoCapture, CAP_PROP_FRAME_COUNT
from numpy import ndarray
from huggingface_hub import snapshot_download

from chutes.chute import Chute, NodeSelector
from chutes.image import Image


##============VARIABLES=======================
HF_REPO_NAME = "{{ huggingface_repository_name }}"
HF_REPO_REVISION = "{{ huggingface_repository_revision }}"
CHUTES_USERNAME = "{{ chute_username }}"
CHUTE_NAME = "{{ chute_name }}"
FILENAME = "miner.py"
CLASSNAME = "Miner"

##=========DATA CLASSES================
class TVPredictInput(BaseModel):
    url: str
    meta:dict[str, Any] = {}

class TVPredictOutput(BaseModel):
    success: bool
    predictions: dict[str, list[dict]] | None = None
    error: str | None = None


##=========UTILITY FUNCTIONS=================
def load_chute_config_from_hf_repo(config_path: Path) -> dict:
    try:
        if not config_path.exists():
            raise ValueError("No config file found in repo")

        with config_path.open() as configuration_file:
            config = safe_load(configuration_file)
            print(f"✅ Loaded Chutes Configuration")
            return config or {}
    except Exception as e:
        print(f"⚠️ Failed to load Chutes Configuration. Using defaults: {e}")
        return {}


def safe_instantiate(cls, config: dict):
    params = {k: v for k, v in config.items() if k in signature(cls).parameters}
    print(
        f"The following parameters will be applied when instantiating {cls.__name__}: {params}"
    )
    obj = cls(**params)
    for function_name, value in config.items():
        if not hasattr(obj, function_name) or not callable(getattr(obj, function_name)):
            continue
        print(f"applying .{function_name}({value})")
        method = getattr(obj, function_name)
        if isinstance(value, list):
            for v in value:
                if isinstance(v, (tuple, list)):
                    method(*v)
                else:
                    method(v)
        else:
            method(value)
    return obj


def load_chute(
    hf_repo: str, hf_revision: str, config_filename:str,  chutes_username:str, chute_name:str, 
) -> Chute:
    hf_repo_path = Path(snapshot_download(hf_repo, revision=hf_revision))
    print("✅ Huggingface Hub repo downloaded")

    config = load_chute_config_from_hf_repo(config_path = hf_repo_path / config_filename)
    print("✅ Config file loaded")

    image_config = config.get("Image", {})
    image_config.update(
        {
            "username": chutes_username, 
            "name": chute_name,
            "tag":"latest",
        }
    )
    chute_image = safe_instantiate(cls=Image, config=image_config)
    print("✅ Image instantiated")

    machine_specs = safe_instantiate(
        cls=NodeSelector, config=config.get("NodeSelector", {})
    )
    print("✅ NodeSelector instantiated")

    chute_config = config.get("Chute", {})
    chute_config.update(
        {
            "username": chutes_username,
            "name": chute_name,
            "image": chute_image,
            "node_selector": machine_specs,
            "allow_external_egress":False
        }
    )
    chute = safe_instantiate(cls=Chute, config=chute_config)
    print("✅ Chute instantiated")
    return chute


def load_miner_from_hf_repo(path_hf_repo: Path, filename: str, classname: str):
    path = path_hf_repo / filename
    if not path.is_file():
        raise FileNotFoundError(
            f"❌ Required file '{filename}' not in repo {path_hf_repo}"
        )

    spec = spec_from_file_location("miner", path)
    module = module_from_spec(spec)
    sys.modules["miner"] = module
    spec.loader.exec_module(module)
    miner_object = getattr(module, classname)
    return miner_object


def get_video_frames_in_batches(
    video: VideoCapture, batch_size: int
) -> Generator[list[ndarray], None, None]:
    batch = []
    while True:
        ok, frame = video.read()
        if not ok:
            if batch:
                yield batch
            break

        batch.append(frame)

        if len(batch) >= batch_size:
            yield batch
            batch = []


##=========ENDPOINTS================
chute = load_chute(
    hf_repo=HF_REPO_NAME, 
    hf_revision=HF_REPO_REVISION, 
    chutes_username=CHUTES_USERNAME, 
    chute_name=CHUTE_NAME,
    config_filename="config.yml",
)

@chute.on_startup()
async def load_model(self) -> None:
    try:
        hf_repo_path = Path(snapshot_download(HF_REPO_NAME, revision=HF_REPO_REVISION))
        print("�~\~E Huggingface Hub repo downloaded")         

        miner_class = load_miner_from_hf_repo(
            path_hf_repo=hf_repo_path, filename=FILENAME, classname=CLASSNAME
        )
        self.miner = miner_class(hf_repo_path)
        print(f"✅ Miner loaded {self.miner}")
    except Exception as e:
        print(f"❌ Failed to load miner from Huggingface repo: {e}")
        self.miner = None


@chute.cord(public_api_path="/health")
async def health(self, *args, **kwargs) -> dict[str, Any]:
    return {
        "status": "healthy",
        "model_loaded": str(self.miner),
    }


@chute.cord(
    public_api_path="/predict",
)
async def predict(self, data: TVPredictInput) -> dict:
    try:
        if self.miner is None:
            raise ValueError(f"Models were not properly loaded!")

        metadata = data.meta
        batch_size = metadata.get('batch_size',64)
        n_keypoints = metadata.get('n_keypoints',32)
        
        url = f"https://proxy.chutes.ai/misc/proxy?url={quote(data.url, safe='')}"
        async with ClientSession() as session:
            async with session.get(url) as response:
                response.raise_for_status()
                content = await response.read()
                print(f"✅ Challenge video downloaded. Saving video to temporary file.")

                with NamedTemporaryFile(prefix="sv_video_", suffix=".mp4") as f:
                    f.write(content)
                    cap = VideoCapture(f.name)
                    if not cap.isOpened():
                        raise ValueError(f"Problem accessing downloaded video {f.name}")

                    n_frames = int(cap.get(CAP_PROP_FRAME_COUNT))
                    print(
                        f"Processing video with {n_frames} frames in batches of {batch_size}"
                    )

                    frame_results = []
                    for batch_number, images in enumerate(
                        get_video_frames_in_batches(video=cap, batch_size=batch_size)
                    ):
                        frame_number = batch_size * batch_number
                        print(f"Predicting Batch: {batch_number+1}")
                        batch_frame_results = self.miner.predict_batch(
                            batch_images=images,
                            offset=frame_number,
                            n_keypoints=n_keypoints,
                        )
                        frame_results.extend(
                            [frame_result.model_dump() for frame_result in batch_frame_results]
                        )

                    cap.release()
                    print(f"✅ Frame Predictions Completed")

        result = TVPredictOutput(success=True, predictions={"frames": frame_results})

    except Exception as e:
        result = TVPredictOutput(
            success=False,
            error=f"❌ There was a problem during prediction: {e}",
        )
    return result.model_dump(mode="json")
